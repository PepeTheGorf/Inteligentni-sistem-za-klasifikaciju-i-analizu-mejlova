{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "",
   "id": "979ce3fa89428bc4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Objašnjenje Koda za Email Spam Klasifikaciju\n",
    "\n",
    "## Uvod\n",
    "\n",
    "Ovaj kod implementira sistem za detekciju spam email-ova koristeći tri popularna transformer modela: BERT, DistilBERT i RoBERTa. Sistem trenira modele na Enron datasetu i testira ih na različitim datasetima."
   ],
   "id": "61d3e344ac27b63c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T18:24:52.487304Z",
     "start_time": "2025-10-16T18:24:00.700815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, f1_score, \\\n",
    "    recall_score, confusion_matrix"
   ],
   "id": "46e87ce66c285808",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\boris\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Koristimo **PyTorch** kao backend framework, **Transformers** biblioteku od Hugging Face za pristup pre-treniranim modelima, i **datasets** biblioteku za efikasno rukovanje podacima.\n",
   "id": "eafd5cf9d73b44fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "MODEL_CONFIGS = {\n",
    "    'bert': {\n",
    "        'name': 'bert-base-uncased',\n",
    "        'tokenizer': BertTokenizer,\n",
    "        'model': BertForSequenceClassification\n",
    "    },\n",
    "    'distilbert': {\n",
    "        'name': 'distilbert-base-uncased',\n",
    "        'tokenizer': DistilBertTokenizer,\n",
    "        'model': DistilBertForSequenceClassification\n",
    "    },\n",
    "    'roberta': {\n",
    "        'name': 'roberta-base',\n",
    "        'tokenizer': RobertaTokenizer,\n",
    "        'model': RobertaForSequenceClassification\n",
    "    }\n",
    "}"
   ],
   "id": "bcb98b6859271170"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Definišemo tri modela:\n",
    "- **BERT** - originalni transformer model\n",
    "- **DistilBERT** - lakša verzija BERT-a (brža, manje resursa)\n",
    "- **RoBERTa** - optimizovana verzija BERT-a"
   ],
   "id": "39f138bb5aee1c43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Funkciju za pripremu dataset-ova",
   "id": "68fd7f58ccaeef31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "def prepare_datasets(X_train, y_train, X_val, y_val, X_test, y_test):\n",
   "id": "abca4464c0df18dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pretvara numpy nizove u Hugging Face `Dataset` objekte koji su optimizovani za rad sa transformerima.",
   "id": "683e0befbc3b31a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenizacija",
   "id": "8f689e2bcebad1e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "def preprocess_function(examples, tokenizer, max_length=128):\n",
   "id": "631675d1f402849f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Tokenizacija** pretvara tekst u numeričke tokene koje model može da razume. Postavljamo:\n",
    "- `max_length=128` - maksimalna dužina sekvence\n",
    "- `padding='max_length'` - dopunjavanje kraćih tekstova\n",
    "- `truncation=True` - skraćivanje dužih tekstova"
   ],
   "id": "e85f9aac5f33122d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Metrike evaluacije modela",
   "id": "78c5049420c57e83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "def compute_metrics(eval_pred):\n",
   "id": "a61558dbc5798bdd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Računamo ključne metrike:\n",
    "- **Accuracy** - ukupna tačnost\n",
    "- **F1 Score** - harmonijska sredina preciznosti i odziva\n",
    "- **Precision** - koliko je detektovanih spam-ova zaista spam\n",
    "- **Recall** - koliko smo stvarnih spam-ova detektovali"
   ],
   "id": "49af35f97a4a63a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Treniranje Modela\n",
    "Ova funkcija:\n",
    "- Deli skup podataka na train/val/test\n",
    "- Tokenizuje podatke\n",
    "- Inicijalizuje model i trenira ga pomoću Hugging Face `Trainer` klase\n",
    "- Evaluira performanse i vraća ključne rezultate"
   ],
   "id": "5a293c065b97c419"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T18:47:03.216541Z",
     "start_time": "2025-10-16T18:47:03.207137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_with_trainer(X_train, y_train, X_test, y_test, model_type='bert', use_cross_dataset=True):\n",
    "    config = MODEL_CONFIGS[model_type]\n",
    "    MODEL_NAME = config['name']\n",
    "    TokenizerClass = config['tokenizer']\n",
    "    ModelClass = config['model']\n",
    "\n",
    "    MAX_LENGTH = 128\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 3\n",
    "    LEARNING_RATE = 2e-5\n",
    "    WEIGHT_DECAY = 0.01\n",
    "\n",
    "    if use_cross_dataset:\n",
    "        X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "            X_train, y_train, test_size=0.15, stratify=y_train, random_state=42\n",
    "        )\n",
    "        X_test_final, y_test_final = X_test, y_test\n",
    "    else:\n",
    "        X_temp, X_test_final, y_temp, y_test_final = train_test_split(\n",
    "            X_train, y_train, test_size=0.15, stratify=y_train, random_state=42\n",
    "        )\n",
    "        X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "            X_temp, y_temp, test_size=0.175, stratify=y_temp, random_state=42\n",
    "        )\n",
    "\n",
    "    tokenizer = TokenizerClass.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = prepare_datasets(\n",
    "        X_train_split, y_train_split,\n",
    "        X_val_split, y_val_split,\n",
    "        X_test_final, y_test_final\n",
    "    )\n",
    "\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda x: preprocess_function(x, tokenizer, MAX_LENGTH),\n",
    "        batched=True\n",
    "    )\n",
    "    val_dataset = val_dataset.map(\n",
    "        lambda x: preprocess_function(x, tokenizer, MAX_LENGTH),\n",
    "        batched=True\n",
    "    )\n",
    "    test_dataset = test_dataset.map(\n",
    "        lambda x: preprocess_function(x, tokenizer, MAX_LENGTH),\n",
    "        batched=True\n",
    "    )\n",
    "\n",
    "    train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ],
   "id": "8fcc0de15b6b3afc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Rezultat tokenizacije:\n",
    "- input_ids - Niz brojevva koji predstavljaju tokene\n",
    "- attention_mask - Niz koji označava koje tokene model treba da obradi (1) a koje da ignoriše (0)\n",
    "- labels - Prave klase (0 za ham, 1 za spam)"
   ],
   "id": "a40abbdd81478fc1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Trening\n",
    "U ovom delu inicijalizujemo model, definišemo parametre treninga i pokrećemo proces treniranja. Nakon treniranja model se evaluira na validacionom i test skupu i čuva u folderu.\n"
   ],
   "id": "17a2fba0f96b69ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    model = ModelClass.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='f1',\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=2,\n",
    "        warmup_steps=500,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    val_results = trainer.evaluate(val_dataset)\n",
    "    test_predictions = trainer.predict(test_dataset)\n",
    "    predicted_labels = np.argmax(test_predictions.predictions, axis=1)\n",
    "    true_labels = test_dataset['label']\n",
    "\n",
    "    report = classification_report(\n",
    "        true_labels, predicted_labels,\n",
    "        target_names=['ham', 'spam'], zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(true_labels, predicted_labels)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predicted_labels, average='binary', pos_label=1, zero_division=0\n",
    "    )\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    model_save_path = f'./fine_tuned_{model_type}_spam_classifier'\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"trainer\": trainer,\n",
    "        \"val_results\": val_results,\n",
    "        \"test_report\": report,\n",
    "        \"test_metrics\": {\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec,\n",
    "            \"f1\": f1,\n",
    "            \"confusion_matrix\": cm\n",
    "        }\n",
    "    }"
   ],
   "id": "e54dc366f09d20b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Pokretanje treninga i prikaz rezultata\n",
    "U ovoj ćeliji treniramo model i prikazujemo ključne metrike (accuracy, precision, recall, F1, confusion matrix).\n"
   ],
   "id": "4d7088332b70acb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main():\n",
    "    df_enron = pd.read_csv(\"/content/drive/MyDrive/EmailDatasets/enron_mails.csv\").dropna(subset=['Message'])\n",
    "    X_enron = df_enron['Message'].values\n",
    "    y_enron = df_enron['Spam/Ham'].map({'ham': 0, 'spam': 1}).values\n",
    "\n",
    "    df_venky = pd.read_csv(\"/content/drive/MyDrive/EmailDatasets/venky_spam_ham_dataset.csv\").drop(subset=['text'])\n",
    "    X_venky = df_venky['text'].values\n",
    "    y_venky = df_venky['label'].map({'ham': 0, 'spam': 1}).values\n",
    "\n",
    "    SELECTED_MODEL = 'distilbert'\n",
    "    USE_CROSS_DATASET = True\n",
    "\n",
    "    model, tokenizer, trainer = train_with_trainer(\n",
    "        X_enron, y_enron,\n",
    "        X_venky, y_venky,\n",
    "        model_type=SELECTED_MODEL,\n",
    "        use_cross_dataset=USE_CROSS_DATASET\n",
    "    )\n",
    "    return model, tokenizer, trainer"
   ],
   "id": "5af0572d5a806b2f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
